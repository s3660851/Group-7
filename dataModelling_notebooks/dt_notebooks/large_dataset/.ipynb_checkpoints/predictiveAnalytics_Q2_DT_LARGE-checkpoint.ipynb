{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import math\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving the data\n",
    "parkingdata = pd.read_csv(\"parking_duration_of_parking_event_vs_street_ID.csv\")\n",
    "parkingdata = parkingdata.sample(n=10000)\n",
    "parkingdata.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the feature attributes to uppercase for uniformity\n",
    "\n",
    "parkingdata['Area Name'] = parkingdata['Area Name'].str.upper()\n",
    "parkingdata['Street Name'] = parkingdata['Street Name'].str.upper()\n",
    "parkingdata['Between Street 1'] = parkingdata['Between Street 1'].str.upper()\n",
    "parkingdata['Between Street 2'] = parkingdata['Between Street 2'].str.upper()\n",
    "parkingdata['Street Marker'] = parkingdata['Street Marker'].str.upper()\n",
    "parkingdata['Sign'] = parkingdata['Sign'].str.upper()\n",
    "\n",
    "parkingdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all redundant extra whitespaces\n",
    "for x in parkingdata.columns:\n",
    "    if parkingdata[x].dtype == object:\n",
    "        parkingdata[x] = parkingdata[x].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting object to datetime\n",
    "parkingdata['Arrival Time'] = pd.to_datetime(parkingdata['Arrival Time'])\n",
    "parkingdata['Departure Time'] = pd.to_datetime(parkingdata['Departure Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target field identification\n",
    "duration = parkingdata['Duration of Parking Event (in seconds)'].copy()\n",
    "\n",
    "print('Minimum parking duration (seconds): ' + str(duration.min()))\n",
    "print('Maximum parking duration (seconds): ' + str(duration.max()))\n",
    "print('\\nMinimum parking duration (hour): ' + str(duration.min()/3600))\n",
    "print('Maximum parking duration (hour): ' + str(duration.max()/3600))\n",
    "print('\\nMinimum parking duration is under [' + str(math.trunc(duration.min()/3600) + 1) + '] hours')\n",
    "print('Maximum parking duration is under [' + str(math.trunc(duration.max()/3600) + 1) + '] hours')\n",
    "\n",
    "durList = []\n",
    "static = 1\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for record in duration.values:\n",
    "    hour = 3600\n",
    "    index = 0\n",
    "    while static == 1:\n",
    "        if record < hour:\n",
    "#             print(\"\\nDuration in seconds: \" + str(record))\n",
    "            data = math.trunc(record/3600) + 1\n",
    "            durList.append(data)\n",
    "#             print(\"Duration is under \" + str(data) + \" hours\")\n",
    "            break\n",
    "        else:\n",
    "            hour += 3600\n",
    "\n",
    "df.insert(0, 'Duration (Hours)', durList, True)\n",
    "target = df['Duration (Hours)']\n",
    "target.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary Target Features identification (Pre-Hill Climbing)\n",
    "\n",
    "#Convert possible targets to dtype = int\n",
    "intDf = parkingdata.copy()\n",
    "#Drop useless fields\n",
    "intDf.drop('Arrival Time', axis = 1, inplace = True)\n",
    "intDf.drop('Departure Time', axis = 1, inplace = True)\n",
    "intDf.drop('Street Marker', axis = 1, inplace = True)\n",
    "#Drop target field\n",
    "intDf.drop('Duration of Parking Event (in seconds)', axis = 1, inplace = True)\n",
    "#Drop already int fields (Will add back after)\n",
    "intDf.drop('In Violation?', axis = 1, inplace = True)\n",
    "intDf.drop('Side Of Street', axis = 1, inplace = True)\n",
    "intDf.drop('Street ID', axis = 1, inplace = True)\n",
    "intDf.drop('Device ID', axis = 1, inplace = True)\n",
    "\n",
    "#Transform the remainder non-int fields' values to unique int identifiers\n",
    "for column in intDf:\n",
    "    unique_vals = intDf[column].unique()\n",
    "    intDf[column].replace(to_replace = unique_vals, value = list(range(len(unique_vals))), inplace = True)\n",
    "\n",
    "#Create dataframe for all the relevant features\n",
    "features = intDf.copy()\n",
    "\n",
    "#Add back int fields (If not the target field)\n",
    "features['In Violation?'] = parkingdata['In Violation?']\n",
    "features['Side Of Street'] = parkingdata['Side Of Street']\n",
    "features['Street ID'] = parkingdata['Street ID']\n",
    "features['Device ID'] = parkingdata['Device ID']\n",
    "#Add back datetime features with only the meaningful subset of data\n",
    "features['Arrival Hour'] = parkingdata['Arrival Time'].dt.hour\n",
    "features['Departure Hour'] = parkingdata['Departure Time'].dt.hour\n",
    "\n",
    "#Table view of features dataframe\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning\n",
    "         ####NOTE: random_split=1 for test_train\n",
    "#Define the parameters to tune and the values to tune to\n",
    "params_dtc = [\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'min_samples_leaf': [None,1, 2, 4, 10],\n",
    "        'max_depth': [None, 4, 10, 15],\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'min_samples_split':[None,5, 8, 10, 12, 14, 16]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_method = RepeatedStratifiedKFold(n_splits = 5, \n",
    "                                    n_repeats = 1, \n",
    "                                    random_state = 1)\n",
    "\n",
    "gs_dtc = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "                      param_grid = params_dtc, \n",
    "                      cv = cv_method,\n",
    "                      verbose = True,\n",
    "                      scoring = 'accuracy',\n",
    "                      n_jobs = -1,\n",
    "                      return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model with the dataset\n",
    "bestModel = gs_dtc.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best values for the  parameters of the model (Standard output)\n",
    "gs_dtc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best values for the  parameters of the model (Formatted output)\n",
    "best_criterion = bestModel.best_estimator_.get_params()['criterion']\n",
    "best_max_depth =  bestModel.best_estimator_.get_params()['max_depth']\n",
    "best_min_samples_leaf =  bestModel.best_estimator_.get_params()['min_samples_leaf']\n",
    "best_min_samples_split =  bestModel.best_estimator_.get_params()['min_samples_split']\n",
    "best_splitter =  bestModel.best_estimator_.get_params()['splitter']\n",
    "\n",
    "print('Best criterion: ', best_criterion)\n",
    "print('Best max_depth: ', best_max_depth )\n",
    "print('Best min_samples_leaf: ', best_min_samples_leaf )\n",
    "print('Best min_samples_split: ', best_min_samples_split)\n",
    "print('Best splitter: ', best_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the parameter configurations from fitting the model with the dataset\n",
    "results_dtc = pd.DataFrame(gs_dtc.cv_results_['params'])\n",
    "results_dtc['test_score'] = gs_dtc.cv_results_['mean_test_score']\n",
    "results_dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Define model with the optimal parameter values AFTER HILL CLIMBING\n",
    "dtc = metric = DecisionTreeClassifier(criterion = best_criterion, \n",
    "                               max_depth = best_max_depth, \n",
    "                               min_samples_leaf = best_min_samples_leaf,\n",
    "                               min_samples_split = best_min_samples_split,\n",
    "                               splitter = best_splitter,\n",
    "                               random_state = 0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hill climbing\n",
    "new_Ind = []\n",
    "cur_MaxScore = 0.0\n",
    "col_num = len(features.columns)\n",
    "col_Ind_Random = shuffle(range(0, col_num), random_state = 1)\n",
    "features_array = features.values\n",
    "\n",
    "for cur_f in range(col_num):\n",
    "    new_Ind.append(col_Ind_Random[cur_f])\n",
    "    newData = features_array[:, new_Ind]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(newData, target, test_size=0.2, random_state=1)\n",
    "    fit = dtc.fit(x_train, y_train)\n",
    "    cur_Score = dtc.score(x_test, y_test)\n",
    "    \n",
    "    if cur_Score < cur_MaxScore:\n",
    "        new_Ind.remove(col_Ind_Random[cur_f])\n",
    "    else:\n",
    "        cur_MaxScore = cur_Score\n",
    "        print (\"Score with \" + str(len(new_Ind)) + \" selected features: \" + str(cur_Score))\n",
    "print(\"\\nIndexs of the desired features\")\n",
    "print(new_Ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hc = pd.DataFrame()\n",
    "for index in new_Ind:\n",
    "    colName = features.columns[index]\n",
    "    features_hc[colName] = features[colName]\n",
    "features_hc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = gs_dtc.fit(features_hc, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_dtc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_criterion_hc = bestModel.best_estimator_.get_params()['criterion']\n",
    "best_max_depth_hc =  bestModel.best_estimator_.get_params()['max_depth']\n",
    "best_min_samples_leaf_hc =  bestModel.best_estimator_.get_params()['min_samples_leaf']\n",
    "best_min_samples_split_hc =  bestModel.best_estimator_.get_params()['min_samples_split']\n",
    "best_splitter_hc =  bestModel.best_estimator_.get_params()['splitter']\n",
    "\n",
    "print('Best criterion: ', best_criterion_hc)\n",
    "print('Best max_depth: ', best_max_depth_hc )\n",
    "print('Best min_samples_leaf: ', best_min_samples_leaf_hc )\n",
    "print('Best min_samples_split: ', best_min_samples_split_hc)\n",
    "print('Best splitter: ', best_splitter_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the parameter configurations from fitting the model with the dataset\n",
    "results_dtc = pd.DataFrame(gs_dtc.cv_results_['params'])\n",
    "results_dtc['test_score'] = gs_dtc.cv_results_['mean_test_score']\n",
    "results_dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = metric = DecisionTreeClassifier(criterion = best_criterion_hc, \n",
    "                               max_depth = best_max_depth_hc, \n",
    "                               min_samples_leaf = best_min_samples_leaf_hc,\n",
    "                               min_samples_split = best_min_samples_split_hc,\n",
    "                               splitter = best_splitter_hc,\n",
    "                               random_state = 0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining training and testing groups\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_hc, target, test_size = 0.5, random_state = 4)\n",
    "\n",
    "#Training the model previously defined\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "#Obtaining and printing out results from the model (Confusion Matrix)\n",
    "predicted = dtc.predict(x_test)\n",
    "cm = confusion_matrix(y_test,predicted)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "#Printing the numerical result of the confusion matrix\n",
    "print(\"\\n[Train/test split] score: {:.5f}\".format(dtc.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat with KFold - creating groups\n",
    "kf = KFold(n_splits = 5, random_state = 4, shuffle = True)\n",
    "\n",
    "\n",
    "#Repeat with KFold - Training model (previously defined) and obtaining its output\n",
    "kFoldTotal = 0\n",
    "for k, (train_index, test_index) in enumerate(kf.split(features_hc)):\n",
    "    x_train, x_test = features_hc.iloc[train_index], features_hc.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    dtc.fit(x_train, y_train)\n",
    "    kFoldTotal += dtc.score(x_test, y_test)\n",
    "    print(\"[fold {0}] score: {1:.5f}\".format(k, dtc.score(x_test, y_test)))\n",
    "\n",
    "#Printing out the results\n",
    "roundedTotal = round(kFoldTotal/5, 5)\n",
    "print(\"\\nDecision Tree mean score [5 folds] = \" + str(roundedTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the tree\n",
    "from sklearn import tree\n",
    "with open('durationTarget.dot', 'w') as f:\n",
    "    f = tree.export_graphviz(dtc, out_file=f, feature_names=None, class_names=None, filled=True, rounded=True, special_characters=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining training and testing groups\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_hc, target, test_size = 0.5, random_state = 4)\n",
    "\n",
    "#Training the model previously defined\n",
    "dtc.fit(x_train, y_train)\n",
    "\n",
    "#Obtaining and printing out results from the model (Confusion Matrix)\n",
    "predicted = dtc.predict(x_test)\n",
    "cm = metrics.confusion_matrix(y_test,predicted)\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "#Printing the numerical result of the confusion matrix\n",
    "print(\"\\n Accuracy score: {:.5f}\".format(dtc.score(x_test, y_test)))\n",
    "misclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "print(\"\\n Misclassification Rate: {:.5f}\".format(misclassification_error))\n",
    "sensitivity = TP / float(FN + TP)\n",
    "print(\"\\n True Positive Rate: {:.5f}\".format(sensitivity))\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"\\n True Negative Rate: {:.5f}\".format(specificity))\n",
    "precision = TP / float(TP + FP)\n",
    "print(\"\\n Precision : {:.5f}\".format(precision))\n",
    "prevalence =  float(FN + TP)/float(TP + TN + FP + FN)\n",
    "print(\"\\n Prevalence : {:.5f}\".format(prevalence))\n",
    "fscore = (2 * precision * sensitivity) / (precision + sensitivity)\n",
    "print(\"\\n F score : {:.5f}\".format(fscore))\n",
    "print(\"\\n False Positive Rate: {:.5f}\".format(1-specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,predicted)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat with KFold - creating groups\n",
    "kf = KFold(n_splits = 5, random_state = 4, shuffle = True)\n",
    "\n",
    "\n",
    "#Repeat with KFold - Training model (previously defined) and obtaining its output\n",
    "kFoldTotal = 0\n",
    "for k, (train_index, test_index) in enumerate(kf.split(features_hc)):\n",
    "    x_train, x_test = features_hc.iloc[train_index], features_hc.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    dtc.fit(x_train, y_train)\n",
    "    kFoldTotal += dtc.score(x_test, y_test)\n",
    "    print(\"[fold {0}] score: {1:.5f}\".format(k, dtc.score(x_test, y_test)))\n",
    "\n",
    "#Printing out the results\n",
    "roundedTotal = round(kFoldTotal/5, 5)\n",
    "print(\"\\nKNN mean score [5 folds] = \" + str(roundedTotal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
